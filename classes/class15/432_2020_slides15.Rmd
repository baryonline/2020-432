---
title: "432 Class 15 Slides"
author: "github.com/THOMASELOVE/2020-432"
date: "2020-03-19"
output:
  beamer_presentation: 
    colortheme: lily
    fonttheme: structurebold
    keep_tex: yes
    theme: Madrid
---

```{r set-options, include = FALSE}
knitr::opts_chunk$set(comment=NA)
options(width = 60)
```

## Setup

```{r, warning = FALSE, message = FALSE}
library(here); library(magrittr); library(janitor)

library(knitr)
library(MASS)
library(robustbase)
library(quantreg)
library(lmtest)
library(sandwich)
library(boot)
library(rms)
library(broom)
library(tidyverse)

decim <- function(x, k) format(round(x, k), nsmall=k)
theme_set(theme_bw())
```


## Today's Materials

- The `crimestat` data
- Robust Linear Regression Methods 
    - with Huber weights
    - with bisquare weights (biweights)
    - Bounded Influence Regression & Least Trimmed Squares
    - Penalized Least Squares using `ols` in `rms` package
    - Quantile Regression on the Median

# The `crimestat` data and an OLS fit

## The `crimestat` data set

For each of 51 states (including the District of Columbia), we have the state's ID number, postal abbreviation and full name, as well as:

- **crime** - the violent crime rate per 100,000 people
- **poverty** - the official poverty rate (% of people living in poverty in the state/district) in 2014
- **single** - the percentage of households in the state/district led by a female householder with no spouse present and with her own children under 18 years living in the household in 2016

## The `crimestat` data set

```{r, message = FALSE}
crimestat <- read_csv("data/crimestat.csv")
crimestat
```

## Modeling `crime` with `poverty` and `single`

Our main goal will be to build a linear regression model to predict **crime** using centered versions of both **poverty** and **single**.

```{r}
crimestat <- crimestat %>%
    mutate(pov_c = poverty - mean(poverty),
           single_c = single - mean(single))
```

## Our original (OLS) model

Note the sneaky trick with the outside parentheses...

```{r}
(mod1 <- lm(crime ~ pov_c + single_c, data = crimestat))
```

## Coefficients?

```{r}
tidy(mod1, conf.int = TRUE) %>%
  select(term, estimate, std.error, 
         p.value, conf.low, conf.high) %>%
  kable(digits = 3)
```

## OLS Residuals

```{r, echo = FALSE, fig.height = 5}
par(mfrow=c(1,2))
plot(mod1, which = c(1:2))
par(mfrow = c(1,1))
```

Which points are highlighted here?

## Remaining Residual Plots from OLS

```{r, echo = FALSE, fig.height = 5}
par(mfrow=c(1,2))
plot(mod1, which = c(3, 5))
par(mfrow = c(1,1))
```

So which points are of special interest?

## Which points are those?

```{r}
crimestat %>%
  slice(c(2, 9, 25))
```

# Robust Linear Regression with Huber Weights

## Robust Linear Regression with Huber weights

There are several ways to do robust linear regression using M-estimation, including weighting using Huber and bisquare strategies.

- Robust linear regression here will make use of a method called iteratively re-weighted least squares (IRLS) to estimate models. 
- M-estimation defines a weight function which is applied during estimation. 
- The weights depend on the residuals and the residuals depend on the weights, so an iterative process is required.

We'll fit the model, using the default weighting choice: what are called Huber weights, where observations with small residuals get a weight of 1, and the larger the residual, the smaller the weight. 

### Our robust model (using `MASS::rlm`)

```{r}
rob.huber <- rlm(crime ~ pov_c + single_c, data = crimestat)
```

## Summary of the robust (Huber weights) model

```{r}
tidy(rob.huber) %>%
  kable(digits = 3)
```

Now, *both* predictors appear to have estimates that exceed twice their standard error. So this is a very different result than ordinary least squares gave us.

## Glance at the robust model (vs. OLS)

```{r}
glance(mod1)
glance(rob.huber)
```

## Understanding the Huber weights a bit

Let's augment the data with results from this model, including the weights used.

```{r}
crime_with_huber <- augment(rob.huber, crimestat) %>%
    mutate(w = rob.huber$w) %>% arrange(w) %>% tbl_df

head(crime_with_huber, 3)
```

## Are cases with large residuals down-weighted?

```{r, fig.height = 4}
ggplot(crime_with_huber, aes(x = w, y = abs(.resid))) +
    geom_label(aes(label = state)) 
```

## Conclusions from the Plot of Weights

- The district of Columbia will be down-weighted the most, followed by Alaska and then Nevada and Mississippi. 
- But many of the observations will have a weight of 1. 
- In ordinary least squares, all observations would have weight 1.
- So the more cases in the robust regression that have a weight close to one, the closer the results of the OLS and robust procedures will be.

## summary(rob.huber)

```{r, echo = FALSE}
summary(rob.huber)
```

# Robust Linear Regression with the bisquare weighting function

## Robust Linear Regression with the biweight

As mentioned there are several possible weighting functions - we'll next try the biweight, also called the bisquare or Tukey's bisquare, in which all cases with a non-zero residual get down-weighted at least a little. Here is the resulting fit...

```{r}
(rob.biweight <- rlm(crime ~ pov_c + single_c,
                    data = crimestat, psi = psi.bisquare))
```

## Coefficients and Standard Errors

```{r}
tidy(rob.biweight) %>% kable(digits = 3)
```

## Understanding the biweights weights a bit

Let's augment the data, as above

```{r}
crime_with_biweights <- augment(rob.biweight, crimestat) %>%
    mutate(w = rob.biweight$w) %>% arrange(w) %>% tbl_df

head(crime_with_biweights, 3)
```

## Relationship of Weights and Residuals

```{r, fig.height = 4}
ggplot(crime_with_biweights, aes(x = w, y = abs(.resid))) +
    geom_label(aes(label = state)) 
```

## Conclusions from the biweights plot

Again, cases with large residuals (in absolute value) are down-weighted generally, but here, Alaska and Washington DC receive no weight at all in fitting the final model.

- We can see that the weight given to DC and Alaska is dramatically lower (in fact it is zero) using the bisquare weighting function than the Huber weighting function and the parameter estimates from these two different weighting methods differ. 
- The maximum weight (here, for Alabama) for any state using the biweight is still slightly smaller than 1.

## summary(rob.biweight)

```{r, echo = FALSE}
summary(rob.biweight)
```

## Comparing OLS and the two weighting schemes

```{r}
glance(mod1) # OLS
```

## Comparing OLS and the two weighting schemes

```{r}
glance(rob.biweight) # biweights
glance(rob.huber) # Huber weights
```

# Bounded-Influence Regression

## Bounded-Influence Regression and Least-Trimmed Squares

Under certain circumstances, M-estimators can be vulnerable to high-leverage observations, and so, bounded-influence estimators, like least-trimmed squares (LTS) regression have been proposed. The biweight that we have discussed is often fitted as part of what is called an MM-estimation procedure, by using an LTS estimate as a starting point. 

The `ltsReg` function, which is part of the `robustbase` package (Note: **not** the `ltsreg` function from `MASS`) is what I use below to fit a least-trimmed squares model. The LTS approach minimizes the sum of the *h* smallest squared residuals, where *h* is greater than *n*/2, and by default is taken to be (*n* + *p* + 1)/2.

### Least Trimmed Squares Model

```{r}
lts1 <- ltsReg(crime ~ pov_c + single_c, data = crimestat)
```

## Summarizing the LTS model

```{r}
summary(lts1)$coeff
```

## MM estimation

Specifying the argument `method="MM"` to `rlm` requests bisquare estimates with start values determined by a preliminary bounded-influence regression, as follows...

```{r}
rob.MM <- rlm(crime ~ pov_c + single_c, 
              data = crimestat, method = "MM")

glance(rob.MM)
```

## summary(rob.MM)

```{r, echo = FALSE}
summary(rob.MM)
```

# Penalized Least Squares

## Penalized Least Squares with `rms`

We can apply a penalty to least squares directly through the `ols` function in the `rms` package. 

```{r}
d <- datadist(crimestat)
options(datadist = "d")
pls <- ols(crime ~ pov_c + single_c, penalty = 1, 
            data = crimestat, x=T, y = T)
```

## The `pls` fit

```{r, echo = FALSE}
pls
```

## Continuing the `pls` output

```

      Coef     S.E.    t     Pr(>|t|)
 Intercept 364.4059 22.2814 16.35 <0.0001 
 pov_c      15.8488  9.1387  1.73 0.0893  
 single_c   23.6857 17.4723  1.36 0.1816  
```


## How to Choose the Penalty in Penalized Least Squares?

The problem here is how to choose the penalty - and that's a subject I'll essentially skip today. The most common approach (that we've seen with the lasso) is cross-validation.

Meanwhile, what do we conclude about the fit here from AIC and BIC?

```{r}
AIC(pls); BIC(pls)
```

# Quantile Regression (on the Median)

## Quantile Regression on the Median

We can use the `rq` function in the `quantreg` package to model the **median** of our outcome (violent crime rate) on the basis of our predictors, rather than the mean, as is the case in ordinary least squares.

```{r}
rob.quan <- rq(crime ~ pov_c + single_c, data = crimestat)

glance(rob.quan)
```

## summary(rob.quan)

```{r, echo = FALSE}
summary(rob.quan <- rq(crime ~ pov_c + single_c, data = crimestat))
```

## Estimating a different quantile (tau = 0.70)

In fact, if we like, we can estimate any quantile by specifying the `tau` parameter (here `tau` = 0.5, by default, so we estimate the median.)

```{r}
(rob.quan70 <- rq(crime ~ pov_c + single_c, tau = 0.70,
                  data = crimestat))
```

# Conclusions

## Comparing Five of the Models

**Estimating the Mean**

Fit | Intercept CI | `pov_c` CI | `single_c` CI 
---------: | ----------: | ----------: | ----------:  
OLS | (`r 364.4 - 2*22.9`, `r 364.4 + 2*22.9`) | (`r 16.11 - 2*9.62`, `r 16.11 + 2*9.62`) | (`r 23.84 - 2*18.38`, `r decim(23.84 + 2*18.38,2)`) 
Robust (Huber) | (`r decim(343.8 - 2*11.9,1)`, `r 343.8 + 2*11.9`) | (`r 11.91 - 2*5.51`, `r 11.91 + 2*5.51`) | (`r 30.99 - 2*10.53`, `r 30.99 + 2*10.53`) 
Robust (biweight) | (`r 336.1 - 2*12.7`, `r 336.1 + 2*12.7`) | (`r decim(10.32 - 2*5.31,2)`, `r 10.32 + 2*5.31`) | (`r 34.71 - 2*10.16`, `r 34.71 + 2*10.16`) 
Robust (MM) | (`r decim(336.4 - 2*13.2,1)`, `r 336.4 + 2*13.2`) | (`r decim(10.56 - 2*5.53,2)`, `r 10.56 + 2*5.53`) | (`r 32.78 - 2*10.58`, `r 32.78 + 2*10.58`) 

**Note**: CIs estimated for OLS and Robust methods as point estimate $\pm$ 2 standard errors

**Estimating the Median**

Fit | Intercept CI | `pov_c` CI | `single_c` CI | AIC | BIC
-----------------: | ----------: | ----------: | ----------: 
Quantile (Median) Reg | (336.9, 366.2) | (3.07, 28.96) | (4.46, 48,19) 

## Comparing AIC and BIC


Fit | AIC | BIC
---------: | ----------: | ----------: 
OLS | `r decim(AIC(mod1), 1)` | `r decim(BIC(mod1), 1)`
Robust (Huber) | `r decim(AIC(rob.huber), 1)` | `r decim(glance(rob.huber)$BIC[1], 1)`
Robust (biweight) | `r decim(AIC(rob.biweight), 1)` | `r decim(glance(rob.biweight)$BIC[1], 1)`
Robust (MM) | `r decim(AIC(rob.MM), 1)` | `r decim(glance(rob.MM)$BIC[1], 1)`
Quantile (median) | `r decim(AIC(rob.quan), 1)` | `r decim(glance(rob.quan)$BIC[1], 1)`


## Some General Thoughts

1. When comparing the results of a regular OLS regression and a robust regression for a data set which displays outliers, if the results are very different, you will most likely want to use the results from the robust regression. 
    - Large differences suggest that the model parameters are being highly influenced by outliers. 
2. Different weighting functions have advantages and drawbacks. 
    - Huber weights can have difficulties with really severe outliers.
    - Bisquare weights can have difficulties converging or may yield multiple solutions. 
    - Quantile regression approaches have some nice properties, but describe medians (or other quantiles) rather than means.

## Next Time

Regression on a Count Outcome
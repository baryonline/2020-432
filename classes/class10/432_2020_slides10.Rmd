---
title: "432 Class 10 Slides"
author: "github.com/THOMASELOVE/2020-432"
date: "2020-02-18"
output:
  beamer_presentation:
    theme: "Madrid"
    colortheme: "lily"
    fonttheme: "structurebold"
    fig_caption: FALSE
---

```{r set-options, echo=FALSE, cache=FALSE}
knitr::opts_chunk$set(comment=NA)
options(width = 60)
```

## Setup

```{r, warning = FALSE, message = FALSE}
library(skimr); library(janitor)
library(simputation); library(broom)
library(rms) # note: also loads Hmisc
library(tidyverse)
```

## Today's Materials

- Hormone Therapy and Baseline Cholesterol Levels in the `HERS` clinical trial 

The HERS trial is described in Vittinghoff et al., especially Chapter 4.

## Hormone Therapy and Baseline ldl in the HERS Trial

HERS clinical trial of hormone therapy (`ht`). We're excluding the women with diabetes here.

```{r, message = FALSE}
hers <- read_csv("data/hersdata.csv") %>% clean_names()

hers1 <- hers %>%
    filter(diabetes == "no") %>%
    select(subject, ldl, ht, age, smoking, drinkany, sbp,
           physact, bmi, diabetes)
```

## Data describe `r nrow(hers1)` women without diabetes

```{r}
head(hers1)
```

## The Codebook (n = `r nrow(hers1)`)

Variable   | Description | Missing?
---------: | --------------------------------- | ---
`subject`  | subject code | 0
`ldl`      | LDL cholesterol in mg/dl | 7
`HT`       | factor: hormone therapy or placebo | 0
`age`      | age in years | 0 
`smoking`  | yes or no | 0
`drinkany` | yes or no | 2
`sbp`      | systolic BP in mm Hg | 0
`physact`  | 5-level factor | 0
`bmi`      | body-mass index in kg/m^2^ | 2
`diabetes` | yes or no (all of these are no) | 0

## Our Modeling Goal

Predict `ldl` using 

- `age`
- `smoking`
- `drinkany`
- `sbp`
- `physact`
- `bmi`
- the interaction of `smoking` and `bmi`

## Details on `physact` variable

```{r}
hers1 %>% count(physact)
```

## Skim?

```{r, eval = FALSE}
hers1 %>% select(-subject) %>% skim()
```

![](figures/fig01.png)

## Missingness pattern?

```{r}
na.pattern(hers1) # from Hmisc
names(hers1)
```

### Next slide

```{r, eval = FALSE}
par(mfrow = c(2,2))
naplot(naclus(hers1))
par(mfrow = c(1,1))
```

## `naplot(naclus(hers1))`

```{r, echo = FALSE}
par(mfrow = c(2,2))
naplot(naclus(hers1))
par(mfrow = c(1,1))
```


# Simple Imputation into `hers2`

## Simple Imputation for `drinkany`, `bmi` and `ldl`

Since `drinkany` is a factor, we have to do some extra work to impute.

```{r}
set.seed(432092)

hers2 <- hers1 %>%
    mutate(drinkany_n = 
               ifelse(drinkany == "yes", 1, 0)) %>%
    impute_pmm(drinkany_n ~ age + smoking) %>%
    mutate(drinkany = 
               ifelse(drinkany_n == 1, "yes", "no")) %>%
    impute_rlm(bmi ~ age + smoking + sbp) %>%
    impute_rlm(ldl ~ age + smoking + sbp + bmi) 
```

## Now, check missingness...

```{r}
na.pattern(hers2) 
names(hers2)
```

# Multiple Imputation with `aregImpute`

## Multiple Imputation using `aregImpute` from `Hmisc`

Model to predict all missing values of any variables, using additive regression bootstrapping and predictive mean matching.

Steps are:

1. `aregImpute` draws a sample with replacement from the observations where the target variable is observed, not missing. 
2. It then fits a flexible additive model to predict this target variable while finding the optimum transformation of it. 
3. It then uses this fitted flexible model to predict the target variable in all of the original observations.
4. Finally, it imputes each missing value of the target variable with the observed value whose predicted transformed value is closest to the predicted transformed value of the missing value.

## Fitting a Multiple Imputation Model

```{r}
set.seed(4320132)
dd <- datadist(hers1)
options(datadist = "dd")
fit3 <- aregImpute(~ ldl + age + smoking + drinkany +
                       sbp + physact + bmi, 
                   nk = c(0, 3:5), tlinear = FALSE,
                   data = hers1, B = 10, n.impute = 20) 
```


## Multiple Imputation using `aregImpute` from `Hmisc`

`aregImpute` requires specifications of all variables, and several other details:

- `n.impute` = number of imputations, we'll run 20
- `nk` = number of knots to describe level of complexity, with our choice `nk = c(0, 3:5)` we'll fit both linear models and models with restricted cubic splines with 3, 4, and 5 knots
- `tlinear = FALSE` allows the target variable to have a non-linear transformation when `nk` is 3 or more
- `B = 10` specifies 10 bootstrap samples will be used
- `data` specifies the source of the variables


## `aregImpute` Imputation Results (1 of 3)

```{r, eval = FALSE}
fit3
```

![](figures/fig02.png)

## `aregImpute` Imputation Results (2 of 3)

![](figures/fig03.png)

## `aregImpute` Imputation Results (3 of 3)

![](figures/fig04.png)

## A plot of the imputed values... (results)

```{r, echo = FALSE}
par(mfrow = c(1,3))
plot(fit3)
par(mfrow = c(1,1))
```


## A plot of the imputed values... (code)

```{r, eval = FALSE}
par(mfrow = c(1,3))
plot(fit3)
par(mfrow = c(1,1))
```

- For `ldl`, we imputed most of the 7 missing subjects in most of the 20 imputation runs to values within a range of around 120 through 200, but occasionally, we imputed values that were substantially lower than 100. 
- For `drinkany` we imputed about 70% no and 30% yes.
- For `bmi`, we imputed values ranging from about 23 to 27 in many cases, and up near 40 in other cases. 
- This method never imputes a value for a variable that doesn't already exist in the data.

## Spearman $\rho^2$ Plot

We've already decided to include a `bmi`*`smoking` product term, but how should we prioritize the degrees of freedom we spend on non-linearity otherwise?

```{r, eval = F}
plot(spearman2(ldl ~ age + smoking + drinkany + sbp + 
                   physact + bmi, data = hers2))
```

## Spearman $\rho^2$ Plot Result

```{r, echo = F}
plot(spearman2(ldl ~ age + smoking + drinkany + sbp + 
                   physact + bmi, data = hers2))
```

# Fitting a Linear Regression with `ols`

## Model we'll fit

Fitting a model to predict `ldl` using

- `bmi` with a restricted cubic spline, 5 knots
- `age` with a quadratic polynomial
- `sbp` as a linear term
- `drinkany` indicator
- `physact` factor
- `smoking` indicator and its interaction with `bmi`

We could fit this to the data

- restricted to complete cases (hers1, effectively)
- after simple imputation (hers2)
- after our multiple imputation (fit3)

## Fitting the model after simple imputation

```{r}
dd <- datadist(hers2)
options(datadist = "dd")

m2 <- ols(ldl ~ rcs(bmi, 5) + pol(age, 2) + sbp + 
              drinkany + physact + smoking + 
              smoking %ia% bmi, data = hers2,
          x = TRUE, y = TRUE)
```

where `%ia%` identifies the linear interaction alone.

## `m2` results (slide 1 of 2)

![](figures/fig05.png)

## `m2` results (slide 2 of 2)

![](figures/fig06.png)

## Validation of summary statistics

```{r}
validate(m2)
```

## `anova(m2)` results

![](figures/fig07.png)

## `summary(m2)` results

![](figures/fig08.png)

## `plot(summary(m2))` results

```{r, echo = FALSE}
plot(summary(m2))
```

## `plot(nomogram(m2))`

```{r, echo = FALSE}
plot(nomogram(m2))
```

## Making Predictions for an Individual

Suppose now that we want to use R to get a prediction for a new individual subject with `bmi` = 30, `age` = 50, `smoking` = yes and `physact` = about as active, `drinkany`= yes and `sbp` of 150.

```{r, eval = FALSE}
predict(m2, expand.grid(bmi = 30, age = 50, smoking = "yes",
                        physact = "about as active", 
                        drinkany = "yes", sbp = 150),
        conf.int = 0.95, conf.type = "individual")
```

```
$linear.predictors        $lower     $upper
          160.9399      88.48615   233.3936
```

## Making Predictions for a Long-Run Mean

The other kind of prediction we might wish to make is for the mean of a series of subjects whose `bmi` = 30, `age` = 50, `smoking` = yes and `physact` = about as active, `drinkany`= yes and `sbp` of 150.

```{r, eval = FALSE}
predict(m2, expand.grid(bmi = 30, age = 50, smoking = "yes",
                        physact = "about as active", 
                        drinkany = "yes", sbp = 150),
        conf.int = 0.95, conf.type = "mean")
```

```
$linear.predictors        $lower     $upper
          160.9399      151.8119   170.0679
```

Of course, the confidence interval will always be narrower than the prediction interval given the same predictor values.

## Influential Points?

```{r}
which.influence(m2, cutoff = 0.4)
```

## Fitting the model to the complete cases

```{r}
d <- datadist(hers1)
options(datadist = "d")

m1 <- ols(ldl ~ rcs(bmi, 5) + pol(age, 2) + sbp + 
              drinkany + physact + smoking + 
              smoking %ia% bmi, data = hers1,
          x = TRUE, y = TRUE)
```

where `%ia%` identifies the linear interaction alone.

# Putting it Together

## What have we got?

- An imputation model `fit3`

```
fit3 <- aregImpute(~ ldl + age + smoking + drinkany + sbp + 
           physact + bmi, nk = c(0, 3:5), tlinear = FALSE,
           data = hers1, B = 10, n.impute = 20, x = TRUE)
```

- A prediction model

```
m1 <- ols(ldl ~ rcs(bmi, 5) + pol(age, 2) + sbp +
            drinkany + physact + smoking + smoking %ia% bmi,
            x = TRUE, y = TRUE)
```

Now we put them together

## Linear Regression & Imputation Model

```{r}
m3imp <- 
    fit.mult.impute(ldl ~ rcs(bmi, 5) + pol(age, 2) + sbp +
                        drinkany + physact + smoking + 
                        smoking %ia% bmi,
                    fitter = ols, xtrans = fit3, 
                    data = hers1)
```

## `m3imp` results (1 of 2)

![](figures/fig09.png)

## `m3imp` results (2 of 2)

![](figures/fig10.png)

## `anova(m3imp)`

![](figures/fig11.png)

## Evaluation via Partial R^2^ and AIC (result)

```{r, echo = FALSE}
par(mfrow = c(1,2))
plot(anova(m3imp), what="partial R2")
plot(anova(m3imp), what="aic")
par(mfrow = c(1,1))
```

## Evaluation via Partial R^2^ and AIC (code)

```{r, eval = FALSE}
par(mfrow = c(1,2))
plot(anova(m3imp), what="partial R2")
plot(anova(m3imp), what="aic")
par(mfrow = c(1,1))
```

## `summary(m3imp)`

![](figures/fig12.png)

## `plot(summary(m3imp))`

```{r, echo = FALSE}
plot(summary(m3imp))
```

## `plot(resid(m3imp) ~ fitted(m3imp))`

```{r, echo = FALSE}
plot(resid(m3imp) ~ fitted(m3imp))
```

## `plot(nomogram(m3imp))`

```{r, echo = FALSE}
plot(nomogram(m3imp))
```
